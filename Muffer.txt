MUFFER PARTNER PRINCIPLES
Speed. Clarity. Ownership.
 We ship outcomes, not hours. If you want hourly safety, this is not for you.
We sell trust.
 Every cut is a promise kept.


Outcomes over effort.
 10 minutes or 10 hours, we pay for delivery quality and speed.


Assigned means the clock starts.
 Once you accept, TAT starts. No “I started later” excuses.


Start early. Always.
 Deadlines kill people who begin at the deadline.


Assume nothing. Ask fast.
 If the brief is unclear, ask within the first 20% of the timeline, not the last 20%.


Follow the brief like a contract.
 Typos, missed guidelines, wrong format, wrong refs equals rework on you.


Revisions are not free labor.
 Two revision cycles are included. Extra revisions are paid only for real scope change.


No ghosting. No drama.
 If you go silent, you fail the job even if the cut is decent.


Your job is not done until it’s approved.
 Deliverable = accepted export link + clean file hygiene + notes.


We reward consistency.
 High TAT, high QC, low revisions, high responsiveness equals more projects and higher rates.
REALITY CHECK
This work is fast, high volume, and deadline-heavy. It’s not “art school vibes.”
It’s more like being a closer.
If that excites you, keep reading.
OPERATING RULES (No debate)
Revisions
Default: 2 revision cycles included in every project.
Paid extra revision: +20% of base rate only if the client changes direction or adds significant scope.
Not paid: revisions caused by missed brief, typos, wrong assets, wrong format, wrong interpretation.
Define “Significant change” (so nobody argues): new story structure, new hooks, new pacing style, new references, new CTA, new footage request, or a new deliverable.
Deadlines and penalties (for unresponsive / no extension request)
Late by 60+ minutes without a valid, verifiable reason and without extension request:
Strike 1: ₹250 deduction
Strike 2: ₹500 deduction
Strike 3: ₹1500 or 20% deduction (whichever is higher) + priority drops
Extension rule: Ask for extension minimum 2 hours before deadline. If you request it after, it’s treated as late unless there’s a real emergency.
Communication
Acknowledge assignment quickly: “Seen. Starting by X. Delivery at Y.”
If you’re stuck, message early with 2 options: “I can do A or B. Pick one.”
Silence = failure. We’d rather you say “I can’t” than disappear.
Scoring (how you earn more work)
We track every project:
TAT (hours)
QC score (1–10)
Revision rounds
Responsiveness
Top performers get first access to better projects and rate upgrades.

WHAT WE PROMISE YOU 
Clear brief + references + deliverable checklist.
Fast QC feedback loop.
Transparent payout per project.
Consistent volume for consistent performers.
If you can’t uphold your side, don’t accept the assignment.

TEST ASSIGNMENT (How to join)
Read this page fully.
Pick a task you prefer (from the list below).
Submit final export link via the form.
If you pass, you get NDA + rate card + onboarding.


General Rules to Follow for Team

Revision Terms

All projects contain two revision cycles by default, extra revision paid at 20% on base. 

Only paid if the changes are significant and from the clients side. 

No extra payouts for revisions due to missed guidelines/typos or misinterpreting the brief. 

If the delivery is late by more than 60 mins than the deadline in scenarios when the editors are unresponsive, ghosting and haven’t requested an extension due to a valid and verifiable reason, there will be a penalty of ₹250 on the first instance that will be deducted from the total payout due on first instance, second delay ₹500 and ₹1500 or 20% will be deducted on the 3rd strike. 

Once the project is accepted by the partner, it is considered assigned, and the clock on TAT score starts right then. 

You are given ample time to furnish the final cut, so if you delay starting till the very end, thinking you will magically get it done in few hours just when the deadline is looming, you will screw up. Poeple always underestimate the time it takes to finish the project and do not account for life stuff, emergencies, electricity cutouts, system crashes and that kills any buffer you had. 

We compensate for outcomes not by hour. So it doesn’t matter if it takes you 10 mins or 10 hours to finish a project but how well you delivered and how fast. 

We value speed and creativity. Speed is easy to understand, the sooner you start the faster you can deliver. The better you plan your assets and timelines, the easier it gets to manage and deliver. 

Creativity is subjective, however, basis the references or brief shared, we can almost perfectly align on the vision and the goals. 




MUFFER = ZOMATO FOR CONTENT
We ship content like Zomato ships food.
Editors + buddies are our delivery partners.
THE DEAL (No BS)
Outcome-based pay. We pay for final output, not hours.
Accept = clock starts. Once assigned, TAT starts.
Start early. Always. Late starts create late deliveries.
Brief is law. Missed guidelines, wrong format, typos = fix on you.
Ask fast. If unclear, ask in the first 20% of timeline.
No ghosting. Silence kills trust.
Done means approved. Job ends on approval, not first export.
REVISIONS
Included: 2 revision cycles.
Extra revision payout: +20% only when client changes scope meaningfully.
No extra payout: when revisions happen due to missed brief, errors, or misinterpretation.
RELIABILITY DEDUCTION (for uncommunicated late delivery)
If you’re 60+ minutes late and didn’t request extension in time:
Strike 1: ₹250
Strike 2: ₹500
Strike 3: ₹1500 or 20% (whichever is higher) + priority drop
Extension rule: request at least 2 hours before deadline.
HOW WE SCORE YOU (QC out of 5)
You’re rated on:
Guideline Accuracy (0–5)
Visual + Audio Quality (0–5)
Self-Reliance (0–5): minimal handholding + proactive clarity
Total QC Score: average of the three.
WHAT YOU GET (our side)
Clear brief + references, fast QC, transparent payouts, and increasing volume for consistent performers.

C. Payment model rewrite (same meaning, sharper, more premium)
Here’s the revised “standard offer mail” section. You can paste it directly.
Payment Model
At Muffer, we operate on output-based compensation. You are paid for approved final deliverables, not hours spent.
Offer Rate
₹30,000 per 60 minutes of fully edited, polished longform content (rate tier depends on complexity and performance)
Rates increase as you maintain on-time streaks, high QC scores, and low revision dependency.
Performance Bonuses
₹1,500 streak bonus for 7 projects delivered on-time in a row with strong QC scores
₹5,000 quality bonus for maintaining 4.8+ average QC over 90 days (minimum project count applies)
Workload Growth
You start small and scale based on:
On-time delivery (TAT)
First-pass acceptance and revision discipline
Guideline adherence and self-reliance
Clean collaboration and responsiveness
Payment Cycle
Payments are processed by the 10th of each month for approved work completed in the previous cycle.
Collaboration Notes
Some projects require coordination with our in-house audio editors
Confidentiality is mandatory
Subtitles must be accurate
Only licensed or original assets are allowed
Project Variety
YouTube videos, reels, digital ads, corporate explainers, product demos.
Why Work With Muffer
Outcome-driven pay that rewards speed + excellence
Freedom and flexibility with a professional system
Long-term partnership for consistent performers
If you’re in, reply “Yes” and we share raw files + brief immediately.

D. Make incentives more exciting (without changing your base promise)
You asked for “industry standards” but your numbers are already fine. What’s missing is a ladder.
Add this as an “exclusive” layer:
The Muffer Pro Ladder (simple tiers)
Starter (first 5 projects): access to basic projects
Pro (after 10 projects + avg QC ≥ 4.6): priority queue + better briefs
Elite (after 25 projects + avg QC ≥ 4.8 + low revision rate): premium projects + rate bumps + rush-slot bonuses
You don’t need to publish exact rate bumps. Just publish eligibility. That creates aspiration.
Add one new bonus (optional but very Zomato-like):
Rush Slot Bonus: +10% payout on projects marked “Rush” (limited slots, offered to Pro/Elite only)


make QC ungameable and consistent
Self-Reliance (0–5) rubric
5: No reminders needed, asks only high-signal questions early, delivers with clean notes, zero chasing


4: Minor clarification needed, responsive, no chasing


3: Needs follow-ups, delayed replies, mild handholding


2: Requires repeated chasing, unclear updates, misses small process steps


1: Ghosting risk behavior, late responses, confusion that could’ve been avoided


0: Non-responsive or breaks process
QC scores are given only after final approval, so editors don’t chase ratings before delivery is complete.
All in Binary. Out of 5
“Self-reliance” rubric (remove subjectivity)
Score is computed from answers, not vibes:
Asked clarifying questions early (Y/N)


Delivered correct exports first time (Y/N)


No missing assets / wrong format (Y/N)


Used reference style correctly (Y/N)


Required >2 back-and-forth messages (Y/N)


Then map checklist → score.
Same for “Guideline accuracy”:
Captions style matched (Y/N)


Safe margins (Y/N)


Brand colors (Y/N)


No typos (Y/N)


Correct duration window (Y/N)


And “AV quality”:
Audio levels clean (Y/N)


No clipping/distortion (Y/N)


Transitions clean (Y/N)


Export quality good (Y/N)


Now the QC score becomes harder to inflate because it’s anchored to explicit checks.



Tally form: section-by-section text + agreement 
Section 1: Intro
“Welcome. Muffer is Zomato for content.
If you love speed, clarity, and ownership, you’ll thrive here.”
Button: Continue
Section 2: QC system 
“We rate every project on three categories (0–5):
Guideline Accuracy
Visual + Audio Quality
Self-Reliance (minimal handholding)
Your cumulative QC score is the average.”
Button: Continue
Section 3: Key terms (short)
Outcome-based pay
Accept = TAT starts
2 revision cycles included
Extra revision payout only for scope change
Reliability deduction for uncommunicated late delivery
Button: Continue
Section 4: Agreement checkboxes (required)
I understand Muffer is output-based pay, not hourly
I understand Accept = TAT clock starts
I understand 2 revision cycles are included per project
I understand extra revision payout applies only for client scope change
I understand revisions due to missed guidelines/typos are unpaid
I understand uncommunicated delay of 60+ minutes triggers reliability deductions
I understand extension requests must be made 2+ hours before deadline
I understand “done” means approved final delivery, not first export
I agree to confidentiality and licensed/original assets only
Section 5: Candidate details
Portfolio, tools, availability, WhatsApp.
Section 6: Test task choice
Pick 1 category + commit submission time.


UGC TEST TASK (Cut Engaging 15 seconds)
Read fully before you start. If you can’t beat the reference, skip.
Your Goal
Create a 15-30s scroll-stopping, attention holding UGC edit in the reference style: fast cuts, bold text emphasis, clean audio, pattern interrupts.

The raw footage may be low quality on purpose. We’re testing your taste + problem-solving.
What You Will Receive
Raw footage + brief
1 reference edit link

Your submission must match the style and upgrade it.
The “Beat the Reference” Rule (Mandatory)
You must improve at least ONE of these clearly:
Hook (first 1–2 seconds)
Pacing (no dead air, tighter cuts)
Captions (cleaner, more readable, better emphasis)
Sound design (voice clarity, tasteful SFX, music ducking)
Visual polish (better grade, cleaner cutout/background, better overlays)
If your edit is “same level” as reference, you will not be shortlisted.

Must-Haves (Non-Negotiable)
Edit & pacing
Pattern interrupt every 2–3 seconds (text hit, b-roll, card, background swap, overlay)
Jump cuts on phrases, not just pauses
Keep energy high, but clarity higher
Text system (keep it simple + clean)
Large emphasis words (high readability)
Clean subtitle flow (no clutter)
Text must stay within safe margins (no UI overlap)
Audio
Voice must be clear and loud enough (no music overpowering)
Music must be ducked under voice
No clipping, no harsh distortion

Submission Deliverables
Final export link (Google Drive preferred)
One-line note: “What I did to beat the reference” (1–2 sentences)
File naming:
UGC_TEST_<YourName>_<Date>.mp4
Export settings (required):
1080x1920 (9:16)
H.264 MP4
25/30 fps
Bitrate: 10–20 Mbps
Audio: 48kHz, AAC

How We Review (So You Know the Game)
We score every test out of 5 in each:
Guideline Accuracy: brief followed, readable text, correct format
Visual + Audio Quality: polish, pacing, sound mix, finesse
Self-Reliance: minimal handholding, smart decisions, clean submission
Instant reject triggers
unreadable text / messy captions
music louder than voice / poor audio quality
random effects that reduce clarity
feels slow or flat (no pattern interrupts)
late submission with no proactive message

Choose Your Submission Deadline (pick one)
(Would be better if its auto-populated based on the today’s day logic with these two options.)
24 hours
48 hours
Only submit if you can commit. Reliability is everything here.
CINEMATIC OR COMEDIC TEST (30 seconds)
Read fully before you start. If you can’t beat the reference, skip.
Your Goal
Create a 30-45s cinematic, storytelling-driven cut from our footage (mostly creative content).
This is not “pretty montage.” It’s emotion + pacing + sound in a micro-story.
What You Will Receive
Raw footage + brief (what the viewer must feel/understand)
1 reference edit link
Your submission must match the vibe and upgrade it.

The “Beat the Reference” Rule (Mandatory)
You must improve at least ONE of these clearly:
Hook (first 1–2 seconds: curiosity, tension, emotion)
Story clarity (beginning → turn → landing)
Rhythm (cuts feel intentional, no filler)
Sound design (world-building, texture, clean mix)
Visual cinema polish (grade, contrast control, intentional framing/crop)
Ending (strong final beat, not abrupt)
If your cut is “same level” as reference, you will not be shortlisted.

Must-Haves (Non-Negotiable)
Story and pacing
Must feel like a complete micro-moment (setup → shift → payoff)
No wasted seconds, no “filler beauty shots”
Cuts should follow emotion and intention, not random beats
Sound design (this matters more than motion graphics)
Clean dialogue/VO if present
Atmos + texture (subtle, not noisy)
Music must support the moment, not overpower it
Basic loudness balance: voice on top, music under, SFX tasteful
Visual polish
Clean exposure/contrast, no muddy blacks or blown highlights
Stabilize where needed
Consistent crop/framing (don’t look accidental)
Minimal text. If you use text, it must be cinematic and intentional.

What NOT to do (instant downgrade)
Meme edits, trend spam, overused presets
Over-editing that kills emotion
Random SFX every cut
Heavy captions like UGC (wrong vibe)
“Cinematic” = slow shots with no story (rejected)

Submission Deliverables
Final export link (Google Drive preferred)
One-line note: “What I did to beat the reference” (1–2 sentences)
Optional (strong plus): 10–20 sec timeline screenshot showing layers for sound design
File naming:
CINE_TEST_<YourName>_<Date>.mp4
Export settings (required):
1080x1920 (9:16) unless brief asks otherwise
H.264 MP4
25/30 fps
Bitrate: 10–20 Mbps
Audio: 48kHz, AAC

How We Review (So You Know the Game)
We score every test out of 5 in each:
Guideline Accuracy: brief followed, correct format, clean deliverable
Visual + Audio Quality: cinema polish + mix + pacing
Self-Reliance: makes smart choices without handholding, clean submission
Instant reject triggers
unclear story or weak hook
poor audio mix or harsh processing
over-stylized grade that looks fake
feels like a montage, not a moment
late submission with no proactive message

Choose Your Submission Deadline (pick one)
24 hours
48 hours
Only submit if you can commit. Reliability is everything here.



Revenue per project Calculation 

minus Editor payout
minus Buddy/QC payout
minus Overheads (ops, tools, management)

= Contribution margin

We track contribution margin project basis and team member basis. 

Rule of thumb to stay profitable:


Keep “Fulfillment Costs” (Editor + QC + Ops) within a fixed % of project revenue (example: 40% cap). Everything else is negotiable, this is not.

Here’s (3) the editor job card payout range math as a clean, standard spec your dev can implement.

Editor Job Card Payout Range Spec (Min–Max)
Goal
Show editors an Uber-style payout range before they accept a job, without leaking client pricing or allowing gaming.
The range is driven by two things:
Reliability Factor (late penalty)
Quality Factor (QC-based bonus/penalty)
Everything else is already known at offer time:
Billable Minutes (BM)
Tier Rate (₹/min)
Any fixed bonuses (rush, missions eligibility)

1) Inputs (known at offer time)
Required
bm_final (float)
tier_rate_per_min (int)
editor_cap_amount (int) (optional to keep hidden from editor, but used in computation)
reliability_bands[] (config)
quality_bands[] (config)
bonus_catalog[] (config)
Optional job flags
is_rush (bool)
rush_bonus_amount (int) and rush_bonus_conditions
difficulty_factor already baked into bm_final
add-ons already baked into bm_final and revenue

2) Base payout
base = bm_final × tier_rate_per_min

Example: BM 1.25 × ₹500/min = ₹625

3) Choose standardized scenarios for the range
To avoid confusion, the range always uses fixed scenario assumptions, not dynamic guesses.
Scenario definitions
Best case (MAX)
Reliability: On-time band factor (usually 1.00)
Quality: Top quality band factor (e.g. 1.05)
Bonus: include eligible fixed bonuses (rush) only if conditions are feasible
For rush bonus in preview: include it only if deadline is within rush window AND editor can still complete (optional: if time-left >= minimum_work_time)
Worst case (MIN)
Reliability: a conservative “likely penalty” band (recommended: late <60m factor 0.95 OR late 60–180m factor 0.85 depending on your desired honesty)
Quality: conservative band (recommended: 0.95 or 0.85)
Bonus: exclude bonuses
Important: Do NOT use the absolute worst (0.70, 0.85) always. That makes ranges scary and reduces accept rate. Use “realistic low”.
Recommended default:
min_reliability_factor = 0.85 (late 60–180m)
min_quality_factor = 0.95 (QC 4.0–4.49)
You can tune these globally.

4) Compute min and max
Gross values
min_gross = base × min_reliability_factor × min_quality_factor
max_gross = base × max_reliability_factor × max_quality_factor

Apply cap (internal)
min_capped = min(min_gross, editor_cap_amount)
max_capped = min(max_gross, editor_cap_amount)

Add bonuses (only to max if eligible)
Bonuses should be fixed amounts, not multipliers:
max_bonus = sum(eligible_bonuses_preview)
min_bonus = 0

Final preview range:
payout_min = round(min_capped + min_bonus)
payout_max = round(max_capped + max_bonus)


5) Bonus preview rules (anti-gaming)
Bonuses must show with clear conditions, but not allow “cheating”.
Rush bonus inclusion in MAX preview
Include rush bonus in max only if:
job is flagged rush OR has “deliver by midnight” incentive
time_left_to_deadline >= minimum_rush_feasibility_buffer (config, e.g. 2 hours)
AND bonus conditions are purely outcome-based (on-time + QC threshold)
Example condition text shown to editor:
“+₹150 if delivered before 11:59 PM and QC ≥ 4.6”
Missions
Do not include mission bonuses in per-job payout range unless the mission is job-specific (rare). Instead show mission progress elsewhere.

6) What to display on the job card (editor-facing)
Always show
Payout Range: ₹X – ₹Y
“Base payout: ₹Z (BM × rate)”
“Why it varies” tooltip bullets:
On-time delivery affects payout
QC rating affects payout
Bonuses apply if conditions met
Optional
Show “Bonus available” badge with exact amount and condition.
Never show
client price
editor cap number (optional)
margin warnings

7) Example using your SKU
UGC Standard 60–90s
BM = 1.25
Tier Rate (Standard) = 500/min
base = 625
cap = 750
Max reliability = 1.00
Max quality = 1.05
Rush bonus = 150 (by midnight + QC≥4.6)
Assume preview mins:
min_reliability = 0.85
min_quality = 0.95
Compute:
min_gross = 625 × 0.85 × 0.95 = 504.7 → 505
max_gross = 625 × 1.00 × 1.05 = 656.25 → 656
cap doesn’t apply (both <750)
Range:
min = 505
max = 656 + 150 = 806
So editor sees:
₹505 – ₹806
With note:
“+₹150 if delivered by 11:59 PM and QC ≥ 4.6”
If you feel ₹806 looks too high vs cap expectations, you have two choices:
Keep bonus outside cap (recommended for pride)
Or set “bonus cap” per SKU (less pride)

8) API response payload (job card)
GET /orders/{id}/jobcard?editor_id=...
{
  "order_id": "o123",
  "sku_name": "UGC Standard 60–90s",
  "deadline_at": "2026-01-19T23:59:00+05:30",
  "billable_minutes": 1.25,
  "tier_rate_per_min": 500,
  "payout_preview": {
    "base": 625,
    "range": { "min": 505, "max": 806 },
    "assumptions": {
      "min_case": { "reliability_factor": 0.85, "quality_factor": 0.95, "bonuses": 0 },
      "max_case": { "reliability_factor": 1.0, "quality_factor": 1.05, "bonuses": 150 }
    },
    "bonuses_preview": [
      {
        "code": "RUSH_MIDNIGHT",
        "amount": 150,
        "condition_text": "Earn +₹150 if delivered before 11:59 PM and QC ≥ 4.6"
      }
    ]
  }
}


9) Tuning knobs (global, simple)
To tune accept rate vs honesty, only adjust:
min_reliability_factor used for preview
min_quality_factor used for preview
bonus feasibility buffer
Everything else stays stable.
The right “two-layer” model:
Outside (Menu Card): every SKU has a known price, a known “work unit” (billable minutes blocks), and a budget (so margin is protected).
Inside (Wholesale pride): partners have tier rates (₹/minute) and can see payout ranges before accepting, like Uber.
The stable payout engine
Definitions (simple)
SKU decides: “What is this item worth?”
It defines billable minutes, price, and budget caps.
Tier decides: “How expensive is this editor?”
It defines rate per minute.
Multipliers decide: “How hard or urgent is this job today?”
Factors decide: “Did they deliver on time and with quality?”
Missions decide: “Extra bonus if certain conditions happen.”

Config you can edit anytime (no code changes)
1) Editor tiers
Each tier has a negotiated rate:
tier_rate_per_minute (Junior 250, Standard 500, Senior 750, Elite 1000 etc)
Optional flags:
rush_eligible
allowed_complexity_max
2) SKU catalog (this is your menu card)
For each SKU:
client_price
std_duration_sec (ex: 45s, 60s)
billable_minutes_base (the “unit” you pay on, not the export length)
overage_billable_min_per_sec (optional)
default_complexity (lite/standard/heavy/cinematic)
editor_budget_pct (ex: 0.35)
incentive_pool_pct (ex: 0.05)
total_multiplier_cap (ex: 1.8)
variant_cap (ex: 3)
3) Multipliers (multiplicative, capped)
complexity_mult: lite 0.9, standard 1.0, heavy 1.25, cinematic 1.5
rush_mult: 1.2
variant_addon: 0.15 (variant mult becomes 1 + v*addon)
optional: uplift_mult (manual boost when you give a junior a “level-up” job)
4) Reliability bands (late minutes → factor)
Example:
0 → 1.00
60 → 0.95
180 → 0.85
9999 → 0.70
5) Quality bands (QC avg → factor)
QC avg is from your 3 categories:
Guidelines accuracy (0–5)
AV quality (0–5)
Self-reliance (0–5)
Then map:
0.0 → 0.85
4.0 → 0.95
4.5 → 1.00
4.8 → 1.05
6) Missions (optional bonus rules)
Each mission has:
name
reward_amount (fixed ₹)
criteria (ex: complete 3 jobs in week, deliver 2 hours early, QC avg >= 4.6)
budget_source: “SKU incentive pool” or “weekly pool”

The payout calculation (developer-ready)
Step 0: QC average
qc_avg = (qc_guidelines + qc_av + qc_self) / 3
Step 1: Billable minutes
This is the “block” unit that makes shortform fair without credits.
bill_min = sku.billable_minutes_base
extra_sec = max(0, actual_duration_sec - sku.std_duration_sec)
bill_min += extra_sec * sku.overage_billable_min_per_sec

Step 2: Base payout
base = bill_min * tier_rate_per_minute
Step 3: Multipliers (multiplicative + caps)
complexity_mult = COMPLEXITY[complexity] (default = sku.default_complexity)
rush_mult = is_rush ? RUSH_MULT : 1

v = min(variants_count, sku.variant_cap)
variant_mult = 1 + v * VARIANT_ADDON

uplift_mult = optional (ex: 1.1 when junior gets a level-up assignment)

total_mult = complexity_mult * rush_mult * variant_mult * uplift_mult
total_mult = min(total_mult, sku.total_multiplier_cap)

gross = base * total_mult

Step 4: Reliability + quality factors
reliability_factor = lookupReliability(late_minutes)
quality_factor = lookupQuality(qc_avg)

computed = gross * reliability_factor * quality_factor

Step 5: SKU budget caps (this is your margin control)
You wanted: protect 50% gross, editor target ~35%, leave 2–10% for incentives.
So:
Editor base cap: cap_editor = sku.client_price * sku.editor_budget_pct
Incentive pool: cap_incentive = sku.client_price * sku.incentive_pool_pct
Now clamp base payout to editor cap, and add mission rewards from incentive pool:
editor_payout = min(round(computed), round(cap_editor))

mission_bonus = min(sum(missions_awarded), round(cap_incentive_remaining_for_this_order_or_period))

final_payout = editor_payout + mission_bonus

Key point: Editors never get surprised because the UI shows “Max payout for this SKU” before they accept.

How your “junior level-up” scenario works
Junior has a lower ₹/min, so you can apply an uplift multiplier and still stay cheaper than Standard.
Example:
SKU billable minutes base: 0.75
Standard tier: ₹500/min → base ₹375
Junior tier: ₹250/min → base ₹188
Uplift: 1.2x → ₹226
Junior feels growth. You still paid less than Standard would cost. And if you hit the SKU editor cap, the system clamps.

What partners should see (Uber-style payout range)
Show 3 numbers on the job card:
Estimated payout now (with current selected multipliers, assuming on-time + QC ≥ 4.5)
Max possible payout (includes best QC factor + any visible mission)
Penalties (late factor table, QC factor table)
This creates pride because it feels like:
“I can earn more if I deliver early, clean, and without handholding.”

Missions and sprints (how to keep it fair and non-gameable)
To avoid “rush trash,” gate mission rewards:
Early-delivery reward only if qc_avg >= 4.6 and revisions <= fix window
Streak reward only if no reliability strikes
Weekly target reward only counts approved jobs
Also keep a simple budget rule:
Mission rewards come from the incentive pool, never from the protected margin.

The one thing you must decide upfront
For each SKU, set:
billable_minutes_base
editor_budget_pct
incentive_pool_pct
Those three numbers are your “truth.” Everything else is just knobs.

Muffer Backend Spec v1
Goal: Productized editing marketplace with (1) wholesale-tier pride, (2) SKU budget control, (3) cross-QC + audits guardrail, (4) configurable without code changes.

0) Core Principles (non-negotiable)
Unit system = Billable Minutes (BM).
BM is set by SKU + add-ons, then adjusted by Difficulty Factor.
Payout engine is stable:
EditorPayout = clamp( BM × TierRate × ReliabilityFactor × QualityFactor , EditorCap ) + Bonuses
Buddy cannot give final numeric QC for their own jobs.
Buddy does Pre-QC only.
Final QC is cross-reviewed + audited (10–20%).
All knobs are config tables, not code.

1) Data Model (tables)
1.1 Users
editors
id (uuid)
name
tier (enum)
tier_rate_per_min (int) // can be derived from tier table but store snapshot optional
is_active (bool)
rush_eligible (bool)
created_at
pms (buddies)
id (uuid)
name
is_active (bool)
reviewer_accuracy (float, 0–1)
open_review_count (int)
created_at

1.2 Config Tables (admin-editable)
tier_rates
tier (enum: junior/standard/senior/elite)
rate_per_min (int)
sku_catalog
sku_code (string, pk)
name
base_price (int)
billable_minutes_base (float)
difficulty_factor_default (float) // 1.0, 1.2, 1.4...
std_duration_sec (int) // optional
editor_budget_pct (float) // e.g. 0.30
ops_budget_pct (float) // e.g. 0.10
incentive_pool_pct (float) // e.g. 0.06
is_active (bool)
add_on_catalog
add_on_code (string, pk)
name
revenue_amount (int)
billable_minutes (float)
is_active (bool)
reliability_bands
min_late_minutes (int) // 0, 60, 180, 9999
factor (float) // 1.00, 0.95, 0.85, 0.70
quality_bands
min_qc_avg (float) // 0.0, 4.0, 4.5, 4.8
factor (float) // 0.85, 0.95, 1.00, 1.05
qc_scoring_rules
category (enum: guideline/av/self)
total_items (int) // 8
thresholds_json (json) // mapping yes_count -> score (configurable)
audit_policy
audit_rate_standard (float) // 0.10
audit_rate_rush (float) // 0.20
audit_rate_new_reviewer (float) // 0.30
gap_ok (float) // 0.30
gap_warn (float) // 0.60
missions
mission_id
name
reward_amount (int)
criteria_json (json)
pool_source (enum: order_incentive_pool / weekly_pool)
is_active (bool)

1.3 Orders and Work
orders
order_id (uuid)
customer_id (uuid/string)
sku_code
add_ons_json (json array of add_on_code)
client_price_total (int)
bm_base (float)
difficulty_factor (float) // default from SKU but can be overridden
bm_final (float)
Budgets (computed at order create)
editor_cap_amount (int)
ops_budget_amount (int)
incentive_pool_amount (int)
incentive_pool_remaining (int)
Workflow
status (enum):
CREATED
OFFERED
ACCEPTED
IN_PROGRESS
SUBMITTED
PREQC_FIX_REQUIRED
SENT_TO_FINAL_QC
FINALQC_FIX_REQUIRED
APPROVED
AUDIT_PENDING
AUDITED
CLOSED
pm_owner_id (uuid) // buddy owner
editor_id (uuid)
editor_tier_at_assign (enum)
tier_rate_at_assign (int)
Timing
created_at
offered_at
accepted_at
deadline_at
submitted_at
approved_at
Derived
late_minutes (int)
revisions_count (int)
scope_change_flag (bool)
escalation_flag (bool)
refund_flag (bool)
customer_rating (float)
Payout
editor_payout_base (int)
editor_bonus (int)
editor_payout_final (int)
pm_fee_base (int)
pm_bonus (int)
pm_payout_final (int)

1.4 QC Data
qc_preqc
order_id
pm_owner_id
outcome (PASS / FIX_REQUIRED)
tags_json (array)
notes (text)
created_at
qc_reviews (Final QC)
order_id
reviewer_pm_id
outcome (APPROVE / FIX_REQUIRED / SCOPE_CHANGE)
guideline_checks_json (8 bools)
av_checks_json (8 bools)
self_checks_json (8 bools)
guideline_score (float)
av_score (float)
self_score (float)
qc_avg (float)
tags_json
notes
created_at
qc_audits
order_id
auditor_id
guideline_checks_json
av_checks_json
self_checks_json
audit_avg (float)
gap (float)
result (OK/WARN/FAIL)
created_at

2) Core Computations (functions)
2.1 Compute billable minutes (at order create)
bm_base = SKU.billable_minutes_base + sum(add_on.billable_minutes)
bm_final = bm_base * difficulty_factor

2.2 Compute budgets (at order create)
editor_cap_amount = round(client_price_total * SKU.editor_budget_pct)
ops_budget_amount = round(client_price_total * SKU.ops_budget_pct)
incentive_pool_amount = round(client_price_total * SKU.incentive_pool_pct)
incentive_pool_remaining = incentive_pool_amount

2.3 Compute QC scores (from checkbox counts)
Use qc_scoring_rules mapping.
Example: yes_count=8 => 5.0, 7=>4.7, 6=>4.3, 5=>3.8, 4=>3.2, else 2.5
Compute:
guideline_score
av_score
self_score
qc_avg = round((g+a+s)/3, 2)
2.4 Lookup factors
reliability_factor = lookupBand(reliability_bands, late_minutes)
quality_factor = lookupBand(quality_bands, qc_avg)
2.5 Compute editor payout (at approval)
base = bm_final * tier_rate_at_assign
score_adj = base * reliability_factor * quality_factor

editor_payout_base = min(round(score_adj), editor_cap_amount)

editor_bonus = computeBonuses(order) 
  - bonuses only paid if incentive_pool_remaining >= bonus
  - also can require qc_avg >= threshold, on_time, etc.

editor_payout_final = editor_payout_base + editor_bonus


3) QC Workflow & Assignment Logic
3.1 Reviewer assignment (cross-review)
When status becomes SUBMITTED and Pre-QC passes:
assign a final reviewer PM automatically.
Eligibility constraints:
reviewer_pm_id != pm_owner_id
reviewer_pm_id not overloaded (open_review_count)
avoid repeated owner-reviewer pairs (cooldown rules)
Selection:
rank by (reviewer_accuracy high, load low)
pick probabilistically from top 5
Store:
final_reviewer_pm_id in order or qc_reviews pending record
3.2 Audit sampling
At APPROVED:
compute audit probability:
rush? use rush rate
reviewer new/low accuracy? use new reviewer rate
else standard rate
if chosen: set AUDIT_PENDING and assign auditor queue
Auditor completes audit:
store gap and result
update reviewer accuracy score (rolling)
move order to CLOSED

4) Endpoints (REST-style)
4.1 Admin/Config
GET /config/all
PUT /config/tier_rates
PUT /config/sku_catalog
PUT /config/add_on_catalog
PUT /config/reliability_bands
PUT /config/quality_bands
PUT /config/qc_scoring_rules
PUT /config/audit_policy
PUT /config/missions
4.2 Orders
Create order
POST /orders
Body
{
  "customer_id": "c123",
  "sku_code": "UGC_STD_90",
  "add_ons": ["VARIANT_1", "RUSH"],
  "pm_owner_id": "pm1",
  "deadline_at": "2026-01-19T23:59:00+05:30"
}

Response
order with computed bm_final and budgets
Get order
GET /orders/{order_id}
Offer order to editor (optional, if you do offers)
POST /orders/{order_id}/offer
{
  "editor_id": "e45"
}

Accept
POST /orders/{order_id}/accept
{ "editor_id": "e45" }

Submit
POST /orders/{order_id}/submit
{
  "export_link": "https://...",
  "notes": "..."
}


4.3 Pre-QC (Buddy)
POST /orders/{order_id}/preqc
{
  "pm_owner_id": "pm1",
  "outcome": "PASS",
  "tags": [],
  "notes": ""
}

If FIX_REQUIRED: status becomes PREQC_FIX_REQUIRED
If PASS: status becomes SENT_TO_FINAL_QC and auto-assign reviewer.

4.4 Final QC (Reviewer PM)
POST /orders/{order_id}/finalqc
{
  "reviewer_pm_id": "pm2",
  "outcome": "APPROVE",
  "guideline_checks": [true,true,true,true,true,true,true,true],
  "av_checks": [true,true,true,true,true,true,true,true],
  "self_checks": [true,true,true,true,true,true,true,true],
  "tags": [],
  "notes": ""
}

Backend computes scores + qc_avg, updates order status:
APPROVE → APPROVED (then audit sampling runs)
FIX_REQUIRED → FINALQC_FIX_REQUIRED
SCOPE_CHANGE → sets scope_change_flag and triggers upsell workflow

4.5 Audits
POST /orders/{order_id}/audit
{
  "auditor_id": "lead1",
  "guideline_checks": [...],
  "av_checks": [...],
  "self_checks": [...]
}


5) Job Card JSON (Editor UX: payout range + warnings)
GET /orders/{order_id}/jobcard?editor_id=e45
Response
{
  "order_id": "o123",
  "sku": {
    "sku_code": "UGC_STD_90",
    "name": "UGC Standard 60-90s",
    "client_price_total": 2500,
    "bm_final": 1.25
  },
  "payout_preview": {
    "tier": "standard",
    "tier_rate_per_min": 500,
    "base_before_factors": 625,
    "range": {
      "min": 531,
      "max": 775
    },
    "assumptions": {
      "min_case": "QC<4.0 or late",
      "max_case": "On-time + QC>=4.6 + rush bonus"
    }
  },
  "budgets": {
    "editor_cap": 750,
    "incentive_pool_remaining": 150,
    "ops_budget": 250
  },
  "warnings": [
    {
      "type": "MARGIN",
      "severity": "YELLOW",
      "message": "Predicted payout may exceed editor cap if bonuses apply. Cap will clamp base payout."
    }
  ],
  "deadline_at": "2026-01-19T23:59:00+05:30"
}

Note: Payout range is computed using:
min: quality_factor_low + reliability_factor_low
max: quality_factor_high + on-time + eligible bonuses

6) PM Assignment UI Spec (Margin Warning + Reason Tags)
When PM selects an editor:
show predicted editor payout (midpoint and worst case)
show cap breach
show traffic light
If breach > X%:
require reason tag
log assignment receipt
Assignment receipt (stored)
order_assignment_log
order_id
pm_owner_id
chosen_editor_id
predicted_payout
editor_cap
cap_breach_amount
reason_tag
created_at

7) Mission System (Sprints/Challenges)
Missions should be separate bonus ledger, never modifying QC or reliability factors.
Mission evaluation trigger
nightly job OR at order close
checks criteria_json
Bonus payout source
default from weekly pool
or per-order incentive pool if mission tied to specific order
Store:
mission_awards
mission_id
recipient_type (editor/pm)
recipient_id
amount
order_id (optional)
created_at

8) Tally Forms (exact fields)
8.1 Editor Onboarding Tally (external)
Sections
Premise (statement)
How payout works (statement)
Agreements (checkboxes)
Preferences (multiple choice)
Portfolio + tool stack
Test selection + submission link
(I can format the exact Tally question text in your preferred tone next.)
8.2 Pre-QC Tally (internal)
Order ID
Outcome PASS/FIX
Fix tags (if FIX)
Notes (3 bullets)
8.3 Final QC Tally (internal)
Order ID
Outcome (approve/fix/scope change)
8 yes/no for each category (Guideline/AV/Self)
Notes + tags
Auditor uses same.

9) Immediate Defaults for Example SKU
UGC Standard 60–90s
client_price_total: 2500
billable_minutes_base: 1.25
difficulty_factor: 1.0
editor_budget_pct: 0.30 → editor_cap 750
ops_budget_pct: 0.10 → 250
incentive_pool_pct: 0.06 → 150
Rush bonus rule (from incentive pool):
if delivered before deadline (midnight) AND qc_avg >= 4.6 → bonus ₹150

1) Roles + Permissions (access levels)
Editor
Can:
View offered/available jobs + payout range
Accept/decline within window
Upload export link + notes
See feedback tags + required fixes
See payout breakdown after approval (and bonuses earned)
Cannot:
See client price or margin
See editor cap amounts (optional, but recommended to hide)
QC score their own work
Buddy (PM Owner)
Can:
Create/ingest orders (or be assigned orders)
Assign editors (discretionary)
Set deadlines + mark rush flag (if sold)
Do Pre-QC gate (PASS / FIX REQUIRED + tags + notes)
Communicate with editor + client
Trigger scope-change request (creates upsell task)
Cannot:
Submit Final QC numeric scoring for their own orders
Override caps without leaving a logged reason
Final QC Reviewer (another PM)
Can:
Final QC screen for orders not owned by them
Approve / Fix Required / Scope Change
Fill rubric (checkboxes) → system computes scores
Cannot:
Review same PM owner too often (cooldown enforced by system)
Auditor / Lead
Can:
Audit queue
Override QC outcome if needed (rare)
View reviewer accuracy and collusion flags
Modify config tables
Admin
Can:
Config: tier rates, SKU BM, difficulty factor, budget %, reliability/quality bands, mission rules
View all dashboards

2) Portal Screens (modules) and required inputs
A) Editor App
1) Job Marketplace View
Each job card shows:
SKU name + deliverables
Deadline + “Rush” tag if applicable
Payout range (min–max) + “why it varies” tooltip
Required output formats (9:16 etc)
Brief + reference links
Actions:
Accept (starts clock)
Decline (optional reason)
Ask question (comment thread)
2) Job Workspace View
Upload export link
Checklist (export format, naming, subtitles, assets)
Notes field (optional)
Status timeline (Accepted → Submitted → Fix window → Approved)
Feedback panel (tags + PM notes)

B) Buddy (PM Owner) App
1) Order Intake / Create
Inputs:
SKU selection
Add-ons selected (variants, rush, etc)
Deadline
Difficulty factor override (rare, but allowed with reason)
Auto computes:
Billable minutes (BM)
Editor cap, ops budget, incentive pool (hidden by default, visible to PM/Lead)
2) Assignment Screen (discretionary + warning label)
PM selects editor and sees:
Predicted payout (mid and worst case)
Traffic light margin warning
“Cap breach amount” (visible to PM/lead)
If red: must select reason tag.
This creates the Assignment Receipt log automatically.
3) Pre-QC Gate Screen (replaces Pre-QC form)
Two buttons:
PASS to Final QC
FIX REQUIRED
If FIX REQUIRED, PM must choose:
Fix tags (multi-select)
Fix notes (3 bullet min)
Fix window type: “Fix window” or “Scope change suspected”
No numeric scores.

C) Final QC Reviewer App
Final QC Screen (replaces Final QC form)
Inputs:
Outcome: Approve / Fix Required / Scope Change
Rubric checkboxes:
Guideline (8 yes/no)
AV quality (8 yes/no)
Self-reliance (8 yes/no/NA)
System computes:
category scores + qc_avg + quality_factor
Outputs:
Approval or fix list
Auto feeds into payout compute

D) Audit App (Lead)
Audit Queue
Shows sampled/flagged jobs
Same rubric checkboxes
System computes:
audit_avg, gap vs reviewer
reviewer_accuracy update
flags if inflation/collusion patterns

3) Updated Workflow (event-driven)
Order created → BM + budgets computed
PM assigns editor → warning + assignment receipt logged
Editor accepts → TAT clock starts
Editor submits → PM notified
PM Pre-QC
Fix required OR pass to final QC
Final QC assigned automatically (random eligible reviewer)
Final QC outcome
Approve → payout computed + optional audit sampling
Fix required → back to editor
Scope change → creates upsell task + locks revision loop
Audit (if sampled) → reviewer accuracy updated
Order closed → editor + PM payouts finalized

4) “Payout range like Uber” (editor-facing logic)
Show editor these:
Base payout (BM × tier rate)
“Quality bonus up to X%” and “late penalty up to Y%”
Eligible bonuses (rush/mission) with conditions
Do NOT show:
client price
editor cap explicitly (optional), but you can show “payout may be capped for this job” as a neutral message
This keeps pride + prevents editors reverse-engineering your pricing too easily.

5) Missions/Sprints Module (Admin + automatic payout)
Admin creates missions like:
“Complete 3 edits this week” = +₹500
“Deliver 2 hours early” = +₹150 (requires QC ≥ 4.6)
Mission engine runs nightly or on order close:
Awards bonus from incentive pool or weekly pool
Shows progress bar in Editor dashboard
